{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnqc+9241vFGTCHqcFKfcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DheviSri/python-lab/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfEJMOsTAb8U",
        "outputId": "2cf366fe-63e4-4b71-cac7-70fe1a77d76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\n"
          ]
        }
      ],
      "source": [
        "num = 100\n",
        "fact = 1\n",
        "while num > 0:\n",
        "    fact = fact * num\n",
        "    num = num - 1\n",
        "\n",
        "print(fact)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\n",
        "for i in range(10):\n",
        "    s += str(i)\n",
        "    print(id(s))\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnBlAXFaAnc9",
        "outputId": "34945ea1-47a1-4431-c238-4d3008ac1e6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134389731255152\n",
            "134389731541552\n",
            "134389730510640\n",
            "134389730510768\n",
            "134389730510832\n",
            "134389730510704\n",
            "134389730510896\n",
            "134389730510960\n",
            "134389730511024\n",
            "134389730511088\n",
            "0123456789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Alternative to save memory space\n",
        "char_list = []\n",
        "for i in range(10):\n",
        "    char_list.append(str(i))\n",
        "s = \"\".join(char_list)\n",
        "print(id(s))\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjC9iiIFAsZD",
        "outputId": "64c807a0-c93e-4241-b3b0-d9826afdc34f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134389730515504\n",
            "0123456789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\u03A9')  # using unicode representation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvdp2XkgAwJ3",
        "outputId": "e6898e0b-2945-4840-9f99-246a221b028e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î©\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\u0001F600')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrRUeG-kAzNy",
        "outputId": "3987e5d6-ad3d-415b-a7a7-b2532435227a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0001F600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata  # using inbuilt library"
      ],
      "metadata": {
        "id": "RRH-oUy4A2TD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "omega = unicodedata.lookup('GREEK CAPITAL LETTER OMEGA')\n",
        "smiley = unicodedata.lookup('GRINNING FACE')"
      ],
      "metadata": {
        "id": "7newb7C3A5Va"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiley  # Directly using keyboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3QCUPejBA-5B",
        "outputId": "d01521ad-ee0e-4579-c792-c924268b1acb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ðŸ˜€'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "omega"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o5G9_w9hBBw3",
        "outputId": "c725b708-a66b-40a3-eab2-860e078235b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Î©'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "character = input(\"Enter the character from keyboard: \")\n",
        "unicode = ord(character)\n",
        "unicode_seq = f'\\\\u{unicode:04x}'\n",
        "print(\"Unicode value of the character is \",unicode_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Eis2jcNBE-F",
        "outputId": "e36ccd28-8016-4f76-cfe7-bf71b81cce47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the character from keyboard: d\n",
            "Unicode value of the character is  \\u0064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Chinese character\n",
        "character = 'ä½ '   # character = 'H'\n",
        "unicode = ord(character)\n",
        "unicode_seq = f'\\\\u{unicode:04x}'\n",
        "print(\"Unicode value of the character is \",unicode_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NWv43qaBKwZ",
        "outputId": "9627bc1b-c30e-44db-8e54-84a3d6bfcd83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unicode value of the character is  \\u4f60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding undocumented character (Tulu language)\n",
        "character = 'à²¤'   # character 'T'\n",
        "unicode = ord(character)\n",
        "unicode_seq = f'\\\\u{unicode:04x}'\n",
        "print(\"Unicode value of the character is \",unicode_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9BJztb4BQMg",
        "outputId": "69d60d6d-7b30-4478-f18e-e1378e309fb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unicode value of the character is  \\u0ca4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "uZAFfRDNBTWa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB085Xj-BXT6",
        "outputId": "33f6e206-125c-467b-8478-3a6791c053c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = '''The dog was called Wellington. It belonged to Mrs Shears who was our friend. She lived on the opposite side of the road, two houses to the left.\n",
        "Wellington was a poodle. Not one of the small poodles that have hairstyles but a big poodle. It had curly black fur, but when you got close you could see that the skin underneath the fur was a very pale yellow, like chicken.\n",
        "'''\n",
        "for text in texts:\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        tagged_words = nltk.pos_tag(words)\n",
        "        print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEbwGWcGBZ5M",
        "outputId": "96169e85-24ee-4e4b-b299-1b0442f36609"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('T', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('g', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('s', 'NN')]\n",
            "[('c', 'NNS')]\n",
            "[('a', 'DT')]\n",
            "[('l', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('W', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('g', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('.', '.')]\n",
            "[('I', 'PRP')]\n",
            "[('t', 'NN')]\n",
            "[('b', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('g', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('M', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('S', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('r', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('s', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('f', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('.', '.')]\n",
            "[('S', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('v', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('p', 'NN')]\n",
            "[('p', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('f', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('d', 'NN')]\n",
            "[(',', ',')]\n",
            "[('t', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('f', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('.', '.')]\n",
            "[('W', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('g', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('s', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('p', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('.', '.')]\n",
            "[('N', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('f', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('m', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('l', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('p', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('v', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('i', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('y', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('b', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('b', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('g', 'NN')]\n",
            "[('p', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('.', '.')]\n",
            "[('I', 'PRP')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('d', 'NN')]\n",
            "[('c', 'NNS')]\n",
            "[('u', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('y', 'NN')]\n",
            "[('b', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('c', 'NNS')]\n",
            "[('k', 'NN')]\n",
            "[('f', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[(',', ',')]\n",
            "[('b', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('y', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('g', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('c', 'NNS')]\n",
            "[('l', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('y', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('c', 'NNS')]\n",
            "[('o', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('t', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('s', 'NN')]\n",
            "[('k', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('d', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('t', 'NN')]\n",
            "[('h', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('f', 'NN')]\n",
            "[('u', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('s', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('v', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('r', 'NN')]\n",
            "[('y', 'NN')]\n",
            "[('p', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('l', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('y', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('l', 'NN')]\n",
            "[('o', 'NN')]\n",
            "[('w', 'NN')]\n",
            "[(',', ',')]\n",
            "[('l', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('k', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('c', 'NNS')]\n",
            "[('h', 'NN')]\n",
            "[('i', 'NN')]\n",
            "[('c', 'NNS')]\n",
            "[('k', 'NN')]\n",
            "[('e', 'NN')]\n",
            "[('n', 'NN')]\n",
            "[('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from urllib import request\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/25344/pg25344.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8')\n",
        "\n",
        "# It's a good practice to remove the Project Gutenberg header/footer\n",
        "# This part is specific to the structure of Project Gutenberg texts\n",
        "start_of_book = \"*** START OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
        "end_of_book = \"*** END OF THE PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\"\n",
        "\n",
        "# Find the start and end of the actual novel content\n",
        "start_index = raw.find(start_of_book)\n",
        "end_index = raw.find(end_of_book)\n",
        "\n",
        "# Extract the novel content\n",
        "if start_index != -1 and end_index != -1:\n",
        "    novel_text = raw[start_index + len(start_of_book):end_index].strip()\n",
        "elif start_index != -1:  # If only start marker is found\n",
        "    novel_text = raw[start_index + len(start_of_book):].strip()\n",
        "else:  # If no markers found, use the raw text (less accurate)\n",
        "    novel_text = raw\n",
        "\n",
        "# Tokenize the novel into sentences\n",
        "sentences = nltk.sent_tokenize(novel_text)\n",
        "\n",
        "# Process and print POS tags for each sentence\n",
        "# For brevity, let's process and print only the first few sentences\n",
        "print(\"POS Tags for the first 5 sentences:\")\n",
        "for i, sentence in enumerate(sentences[:5]): # Process only the first 5 sentences\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tagged_words = nltk.pos_tag(words)\n",
        "    print(f\"Sentence {i+1}: {tagged_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH-xgpU_CVq6",
        "outputId": "221e85ac-fa9e-4565-a33f-b3dcf5e64711"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags for the first 5 sentences:\n",
            "Sentence 1: [('\\ufeffThe', 'NN'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('eBook', 'NN'), ('of', 'IN'), ('The', 'DT'), ('Scarlet', 'NNP'), ('Letter', 'NNP'), ('This', 'DT'), ('ebook', 'NN'), ('is', 'VBZ'), ('for', 'IN'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('anyone', 'NN'), ('anywhere', 'RB'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('most', 'JJS'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('at', 'IN'), ('no', 'DT'), ('cost', 'NN'), ('and', 'CC'), ('with', 'IN'), ('almost', 'RB'), ('no', 'DT'), ('restrictions', 'NNS'), ('whatsoever', 'RB'), ('.', '.')]\n",
            "Sentence 2: [('You', 'PRP'), ('may', 'MD'), ('copy', 'VB'), ('it', 'PRP'), (',', ','), ('give', 'VB'), ('it', 'PRP'), ('away', 'RB'), ('or', 'CC'), ('re-use', 'VB'), ('it', 'PRP'), ('under', 'IN'), ('the', 'DT'), ('terms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Project', 'NNP'), ('Gutenberg', 'NNP'), ('License', 'NNP'), ('included', 'VBD'), ('with', 'IN'), ('this', 'DT'), ('ebook', 'NN'), ('or', 'CC'), ('online', 'NN'), ('at', 'IN'), ('www.gutenberg.org', 'NN'), ('.', '.')]\n",
            "Sentence 3: [('If', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('located', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('you', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('to', 'TO'), ('check', 'VB'), ('the', 'DT'), ('laws', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('country', 'NN'), ('where', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('located', 'VBN'), ('before', 'IN'), ('using', 'VBG'), ('this', 'DT'), ('eBook', 'NN'), ('.', '.')]\n",
            "Sentence 4: [('Title', 'NN'), (':', ':'), ('The', 'DT'), ('Scarlet', 'NNP'), ('Letter', 'NNP'), ('Author', 'NNP'), (':', ':'), ('Nathaniel', 'NNP'), ('Hawthorne', 'NNP'), ('Engraver', 'NNP'), (':', ':'), ('A.', 'NN'), ('V.', 'NNP'), ('S.', 'NNP'), ('Anthony', 'NNP'), ('Illustrator', 'NNP'), (':', ':'), ('Mary', 'NNP'), ('Hallock', 'NNP'), ('Foote', 'NNP'), ('Ludvig', 'NNP'), ('SandÃ¶e', 'NNP'), ('Ipsen', 'NNP'), ('Release', 'NNP'), ('date', 'NN'), (':', ':'), ('May', 'NNP'), ('5', 'CD'), (',', ','), ('2008', 'CD'), ('[', 'NNP'), ('eBook', 'VBD'), ('#', '#'), ('25344', 'CD'), (']', 'NN'), ('Most', 'RBS'), ('recently', 'RB'), ('updated', 'VBN'), (':', ':'), ('July', 'NNP'), ('15', 'CD'), (',', ','), ('2025', 'CD'), ('Language', 'NN'), (':', ':'), ('English', 'JJ'), ('Credits', 'NNS'), (':', ':'), ('Markus', 'NNP'), ('Brenner', 'NNP'), (',', ','), ('Irma', 'NNP'), ('Spehar', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('Online', 'NNP'), ('Distributed', 'NNP'), ('Proofreading', 'NNP'), ('Team', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('START', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('PROJECT', 'NNP'), ('GUTENBERG', 'NNP'), ('EBOOK', 'NNP'), ('THE', 'NNP'), ('SCARLET', 'NNP'), ('LETTER', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'VBD'), ('THE', 'NNP'), ('SCARLET', 'NNP'), ('LETTER', 'NNP'), ('.', '.')]\n",
            "Sentence 5: [('BY', 'NNP'), ('NATHANIEL', 'NNP'), ('HAWTHORNE', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}